# IT Due Diligence Agent Enhancement Plan
## 115-Point Implementation Plan Across 15 Phases

**Version:** 1.0
**Created:** Based on leadership feedback and architecture review
**Scope:** Unknown handling, Excel export, Business context, EOL assessment, Contracts tool, AI explainability

---

## PHASE 1: Foundation & Architecture Planning (8 points)

### Objective: Establish the architectural foundation for all enhancements

1. Review current `AnalysisStore` class structure and identify extension points for new data types
2. Document current enum patterns and establish consistency standards for new enums
3. Map all existing tools in `analysis_tools.py` and their relationships
4. Define the output directory structure for Excel exports and explainability docs
5. Create a shared constants file for cross-cutting values (e.g., "Unknown", "N/A" standards)
6. Establish naming conventions for new tools: `record_*`, `assess_*`, `export_*`
7. Document the tool-to-AnalysisStore data flow pattern for new tool developers
8. Create a test plan template for each phase to ensure consistent quality gates

---

## PHASE 2: Unknown/N/A Handling - Analysis & Design (7 points)

### Objective: Design explicit handling for missing information across all tools

9. Audit all existing enums and identify which already have "Unknown" options
10. Audit `record_application` fields and classify as: Required, Optional-with-Unknown, Optional-omittable
11. Audit `record_capability_coverage` fields for Unknown handling gaps
12. Audit `record_buyer_application` and `record_application_overlap` for consistency
13. Design the `fields_not_documented` pattern: array of field names that couldn't be sourced
14. Define guidance text for when to use "Unknown" vs leaving blank vs flagging a gap
15. Create a decision matrix: field type → Unknown handling approach

---

## PHASE 3: Unknown/N/A Handling - Implementation (8 points)

### Objective: Implement explicit Unknown handling across all application-related tools

16. Add "Unknown" to `HOSTING_MODELS` enum if not present
17. Add "Unknown" to `BUSINESS_CRITICALITY` enum if not present
18. Add "Unknown" to `CUSTOMIZATION_LEVELS` enum if not present
19. Add "Unknown" to `DATA_RESIDENCY_LOCATIONS` enum if not present
20. Add optional `fields_not_documented` array to `record_application` schema
21. Update `record_application` description with explicit guidance on Unknown usage
22. Add validation in AnalysisStore to track completeness metrics per application
23. Write tests verifying Unknown values are accepted and stored correctly

---

## PHASE 4: Excel Export Tool - Design (8 points)

### Objective: Design the Excel export capability for team review

24. Define the Excel workbook structure: one file with multiple tabs
25. Design Tab 1: "Applications" - all recorded applications in table format
26. Design Tab 2: "Capability Coverage" - matrix view of 13 capability areas
27. Design Tab 3: "Buyer Overlaps" - target vs buyer comparison table
28. Design Tab 4: "Risks" - all identified risks with severity and evidence
29. Design Tab 5: "Work Items" - phased work items (Day 1, Day 100, Post 100)
30. Design Tab 6: "Gaps & Questions" - all flagged gaps and follow-up questions
31. Define column headers, data types, and formatting rules for each tab

---

## PHASE 5: Excel Export Tool - Implementation (8 points)

### Objective: Build the Excel export utility

32. Create new file: `tools/export_tools.py`
33. Add `openpyxl` or `xlsxwriter` to project dependencies
34. Implement `export_applications_tab(analysis_store)` function
35. Implement `export_capability_coverage_tab(analysis_store)` function
36. Implement `export_buyer_overlaps_tab(analysis_store)` function
37. Implement `export_risks_tab(analysis_store)` function
38. Implement `export_work_items_tab(analysis_store)` function
39. Implement `export_gaps_questions_tab(analysis_store)` function

---

## PHASE 6: Excel Export Tool - Integration (7 points)

### Objective: Create the unified export function and integrate with workflow

40. Implement `export_analysis_to_excel(analysis_store, output_path)` master function
41. Add formatting: header row styling, column widths, freeze panes
42. Add conditional formatting: red for Critical risks, yellow for High
43. Create summary counts row at top of each tab
44. Add "Generated By" and timestamp footer to each tab
45. Write comprehensive tests for Excel export functionality
46. Document the export tool usage in README

---

## PHASE 7: Business Context Profile - Design (8 points)

### Objective: Design the mechanism for capturing target company business context

47. Define `INDUSTRY_TYPES` enum: Manufacturing, SaaS, Healthcare, Financial_Services, Retail, Professional_Services, etc.
48. Define `BUSINESS_MODELS` enum: B2B, B2C, B2B2C, Marketplace, Subscription, etc.
49. Define `COMPANY_SIZE` enum: Small (<100), Medium (100-1000), Large (1000-5000), Enterprise (5000+)
50. Design `record_business_context` tool schema with fields: industry, business_model, employee_count, revenue_range, geographic_presence, key_business_processes
51. Design the capability-to-industry relevance mapping (e.g., Manufacturing → operations_supply_chain = Critical)
52. Define how business context influences `expected_but_missing` gap detection
53. Design validation rules: business context should be recorded before capability coverage
54. Define public filing information fields: ticker, SEC filings, public company flag

---

## PHASE 8: Business Context Profile - Implementation (8 points)

### Objective: Implement business context capture and relevance mapping

55. Add `INDUSTRY_TYPES` enum to `analysis_tools.py`
56. Add `BUSINESS_MODELS` enum to `analysis_tools.py`
57. Add `COMPANY_SIZE_RANGES` enum to `analysis_tools.py`
58. Implement `record_business_context` tool with full schema
59. Add `business_context` storage to AnalysisStore with getter method
60. Implement `get_expected_capabilities(industry, business_model)` helper function
61. Update `record_capability_coverage` to reference business context for relevance
62. Write tests for business context recording and capability relevance mapping

---

## PHASE 9: Technical Maturity & EOL Assessment - Design (8 points)

### Objective: Design the separate EOL/maturity assessment capability

63. Research and compile EOL dates for common enterprise software (SAP, Oracle, Microsoft, etc.)
64. Define `EOL_STATUS` enum: Current, Approaching_EOL, Past_EOL, Extended_Support, Unknown
65. Define `SUPPORT_STATUS` enum: Mainstream, Extended, Custom_Support, Unsupported, Unknown
66. Design `assess_application_eol` tool schema: app_id, version_detected, eol_status, eol_date, support_status, evidence
67. Design `TECHNOLOGY_RISK_LEVELS` enum for legacy tech assessment
68. Design `assess_technical_debt` tool schema: app_id, debt_factors, modernization_urgency, evidence
69. Define the EOL reference data structure (product → version → EOL date mapping)
70. Design how EOL assessment integrates with risk identification (auto-generate risks for Past_EOL)

---

## PHASE 10: Technical Maturity & EOL Assessment - Implementation (8 points)

### Objective: Implement EOL and technical debt assessment tools

71. Add `EOL_STATUS` and `SUPPORT_STATUS` enums to `analysis_tools.py`
72. Create `EOL_REFERENCE_DATA` dictionary with known EOL dates for major platforms
73. Implement `assess_application_eol` tool
74. Implement `get_eol_status(product, version)` helper that checks reference data
75. Add `eol_assessments` storage to AnalysisStore
76. Implement `assess_technical_debt` tool with debt factors and urgency
77. Add `technical_debt_assessments` storage to AnalysisStore
78. Write tests for EOL assessment and technical debt recording

---

## PHASE 11: Contracts Tool - Design (8 points)

### Objective: Design separate contract information capture capability

79. Define `CONTRACT_TYPES` enum: Software_License, SaaS_Subscription, Maintenance_Support, Professional_Services, Managed_Services
80. Define `CONTRACT_STATUS` enum: Active, Expiring_Soon, Expired, Pending_Renewal, Unknown
81. Design `record_contract` tool schema: vendor, contract_type, applications_covered, start_date, end_date, renewal_terms, change_of_control_clause, annual_value, key_terms
82. Design `CHANGE_OF_CONTROL_IMPACT` enum: Termination_Right, Renegotiation_Required, Price_Increase, No_Impact, Unknown
83. Define contract-to-application linking mechanism (contract covers which app IDs)
84. Design the contract upload/processing workflow (separate from main DD docs)
85. Define contract-specific follow-up questions template
86. Design how contract findings feed into licensing risk analysis

---

## PHASE 12: Contracts Tool - Implementation (8 points)

### Objective: Implement contract recording and analysis tools

87. Add `CONTRACT_TYPES`, `CONTRACT_STATUS`, `CHANGE_OF_CONTROL_IMPACT` enums
88. Implement `record_contract` tool with full schema
89. Add `contracts` storage to AnalysisStore with `add_contract` method
90. Implement `get_contracts()` and `get_contracts_by_vendor()` methods
91. Implement `get_contracts_expiring_soon(days=90)` helper method
92. Implement `get_change_of_control_risks()` to surface contracts with termination rights
93. Update Excel export to add Tab 7: "Contracts" with contract details
94. Write tests for contract recording and retrieval

---

## PHASE 13: AI Explainability - Methodology Documentation (8 points)

### Objective: Create stakeholder-friendly documentation explaining how the AI works

95. Create `docs/AI_METHODOLOGY.md` - plain language explanation of the analysis approach
96. Document the Four-Lens Framework in business terms (what each lens means, why it matters)
97. Document the application enumeration process (how apps are found, recorded, validated)
98. Document the capability coverage approach (what the 13 areas are, how gaps are identified)
99. Document the overlap analysis methodology (how target vs buyer comparison works)
100. Document the EOL/maturity assessment approach (how technical risk is evaluated)
101. Document the evidence requirements (why exact quotes are required, how hallucination is prevented)
102. Create visual diagrams for each major process step

---

## PHASE 14: AI Explainability - Prompt Versioning & Transparency (7 points)

### Objective: Enable tracking and visibility into the prompts driving analysis

103. Add `PROMPT_VERSION` constant to each prompt file (applications_prompt.py, etc.)
104. Add `get_prompt_metadata()` function returning version, last_updated, author
105. Include prompt version in analysis output (so outputs can be tied to specific prompt version)
106. Create `docs/PROMPT_CHANGELOG.md` tracking all prompt changes with rationale
107. Add prompt hash/fingerprint to Excel export metadata tab
108. Document how to view and modify prompts for stakeholders who want to refine
109. Create a "Prompt Summary" one-pager for each domain agent (what it looks for, key frameworks)

---

## PHASE 15: Integration, Testing & Documentation (8 points)

### Objective: Integrate all enhancements and ensure production readiness

110. Run full test suite and ensure all 246+ tests pass
111. Add integration tests covering the full workflow with new tools
112. Update `APPLICATIONS_AGENT_COMPLETE_REFERENCE.md` with all new tools and enums
113. Update `APPLICATION_REVIEW_PROCESS_WALKTHROUGH.md` with new process steps
114. Create quick-start guide for users covering business context, contracts, and exports
115. Conduct end-to-end test with sample DD documents and review outputs with team

---

## Implementation Priority & Dependencies

```
Phase 1 (Foundation)
    ↓
Phase 2-3 (Unknown Handling) ──────────────────────┐
    ↓                                               │
Phase 4-6 (Excel Export) ←─────────────────────────┤
    ↓                                               │
Phase 7-8 (Business Context) ──────────────────────┤
    ↓                                               │
Phase 9-10 (EOL Assessment) ───────────────────────┤
    ↓                                               │
Phase 11-12 (Contracts Tool) ──────────────────────┤
    ↓                                               │
Phase 13-14 (AI Explainability) ───────────────────┘
    ↓
Phase 15 (Integration & Testing)
```

## Estimated Deliverables

| Phase | Primary Deliverable |
|-------|---------------------|
| 1 | Architecture documentation |
| 2-3 | Updated enums with Unknown handling |
| 4-6 | `export_tools.py` with Excel generation |
| 7-8 | `record_business_context` tool |
| 9-10 | `assess_application_eol` and `assess_technical_debt` tools |
| 11-12 | `record_contract` tool |
| 13-14 | `AI_METHODOLOGY.md` and prompt versioning |
| 15 | Updated documentation, full test coverage |

## Success Criteria

- [ ] All existing tests continue to pass
- [ ] New tools have comprehensive test coverage
- [ ] Excel exports are usable by non-technical team members
- [ ] Business context influences capability relevance assessments
- [ ] EOL assessment provides actionable technical debt visibility
- [ ] Contracts are captured separately and linked to applications
- [ ] Stakeholders can understand how the AI works from documentation
- [ ] Prompt versions are tracked and visible in outputs

---

## Notes for Leadership Review

1. **Chunking for large inventories** - Noted for future consideration, not in this plan
2. **File format detection** - Not needed per feedback, LLM handles adequately
3. **Separate contracts upload** - Plan supports this workflow (contracts as distinct document set)
4. **Domain-specific EOL assessment** - Phase 9-10 designed to be extensible per domain

---

*This plan will be executed sequentially with checkpoints after each phase.*
