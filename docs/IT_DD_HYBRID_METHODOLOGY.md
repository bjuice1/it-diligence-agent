# IT Due Diligence Hybrid Methodology

## Executive Summary

This document describes our hybrid AI-assisted approach to IT due diligence cost estimation for M&A transactions. The methodology combines Large Language Model (LLM) interpretation capabilities with a structured, team-maintained knowledge base of activities and cost anchors.

**This approach leverages PwC proprietary knowledge** - our accumulated experience from hundreds of IT carveouts, integrations, and standalone buildouts - codified into a maintainable, auditable format that can be continuously refined by our practitioners.

---

## The Problem We're Solving

### Traditional Approaches Have Limitations

**Pure Expert Judgment:**
- Inconsistent across practitioners
- Difficult to scale
- Knowledge walks out the door
- Hard to explain methodology to clients

**Pure AI/LLM Approaches:**
- "Black box" - difficult to audit or explain
- Hallucination risk on cost estimates
- No institutional knowledge retention
- Prompt engineering is fragile and hard to maintain

**Spreadsheet Templates:**
- Static and quickly outdated
- Don't adapt to deal specifics
- Require significant manual effort per deal
- Limited ability to capture nuance

### Our Solution: Hybrid Methodology

We combine the strengths of each approach while mitigating weaknesses:

| Component | Strength We Leverage |
|-----------|---------------------|
| LLM Interpretation | Reading documents, identifying considerations, understanding context |
| Structured Templates | Consistent cost anchors, auditable, team-maintainable |
| Rule-Based Matching | Predictable, explainable, no hallucination on costs |
| LLM Refinement | Deal-specific adjustments, reasonableness validation |

---

## How It Works

### Three-Stage Architecture

```
┌─────────────────────────────────────────────────────────────────────────┐
│  STAGE 1: LLM Interprets the Deal Environment                          │
│                                                                         │
│  Input:  Deal documents, IT facts, interview notes                      │
│  Action: LLM analyzes and identifies what activities are needed         │
│  Output: List of considerations with categories and implications        │
│                                                                         │
│  Example: "Parent provides identity services via shared Azure AD        │
│           tenant → Need standalone identity platform"                   │
└─────────────────────────────┬───────────────────────────────────────────┘
                              ▼
┌─────────────────────────────────────────────────────────────────────────┐
│  STAGE 2: Match to PwC Activity Inventory (Knowledge Base)             │
│                                                                         │
│  Input:  Stage 1 considerations                                         │
│  Action: Rules match to our template inventory, apply base costs        │
│  Output: Costed activities with formulas and assumptions flagged        │
│                                                                         │
│  This is the STATIC ANCHOR - maintained by our team, not AI prompts    │
│  Contains: 290+ activities, cost ranges, TSA durations, prerequisites   │
└─────────────────────────────┬───────────────────────────────────────────┘
                              ▼
┌─────────────────────────────────────────────────────────────────────────┐
│  STAGE 3: LLM Validates and Refines                                    │
│                                                                         │
│  Input:  Matched activities with base costs                             │
│  Action: Apply deal-specific adjustments, validate reasonableness       │
│  Output: Final costed activity list with confidence indicators          │
│                                                                         │
│  Adjustments: Complexity, industry, geography, timeline pressure        │
└─────────────────────────────────────────────────────────────────────────┘
```

### Why This Architecture Works

**Stage 1 (LLM Interpretation):**
- LLMs excel at reading unstructured documents and understanding context
- Can identify implications that rigid rules would miss
- Handles variety in how information is presented across deals

**Stage 2 (Template Matching):**
- Costs come from our knowledge base, NOT generated by AI
- Eliminates hallucination risk on dollar amounts
- Easy for team to update - just edit the template file
- Fully auditable - can show client exactly where numbers come from

**Stage 3 (LLM Refinement):**
- Applies judgment that's hard to codify in rules
- Validates that the overall estimate makes sense
- Identifies gaps or unusual situations needing human review

---

## The Activity Inventory (Knowledge Base)

### What It Contains

Our inventory is organized by workstream and includes:

| Field | Purpose |
|-------|---------|
| Activity ID | Unique identifier for tracking |
| Name & Description | What the activity involves |
| Cost Model | Base cost, per-user, per-system, etc. |
| Cost Range | Low-high range anchored to market rates |
| Timeline | Typical duration in months |
| TSA Required | Whether transition services are needed |
| TSA Duration | Typical TSA period if required |
| Prerequisites | What must happen first |
| Outputs/Deliverables | What gets produced |
| Notes | Implementation considerations |

### Current Coverage (Phases 1-4)

| Phase | Workstreams | Activities |
|-------|-------------|------------|
| 1 - Core Infrastructure | Identity, Email, Infrastructure | 70 |
| 2 - Network & Security | Network, Security, Perimeter | 67 |
| 3 - Applications | ERP, CRM, HCM, Custom Apps, SaaS | 94 |
| 4 - Data & Migration | Database, Files, Archive, Tooling | 61 |
| **Total** | **15 workstreams** | **292 activities** |

### Planned Coverage (Phases 5-10)

- Phase 5: Licensing (Microsoft, Oracle, VMware, etc.)
- Phase 6: Acquisition & Integration scenarios
- Phase 7: Operational run-rate costs
- Phase 8: Compliance & Regulatory
- Phase 9: Vendor & Contract transition
- Phase 10: Validation & refinement

---

## Why This Is a Good Option

### 1. Maintains Human Control Over Costs

The LLM never generates cost numbers from scratch. All costs trace back to our team-maintained templates. This means:
- No AI hallucination on dollar amounts
- Full audit trail for client discussions
- Team expertise is codified, not replaced

### 2. Easy to Update and Maintain

**Updating a cost anchor:**
```python
# Old
"base_cost": (25000, 50000),

# Updated based on recent deal experience
"base_cost": (30000, 60000),
```

Compare this to trying to update an LLM prompt to "remember" that SAP migrations now cost more. The structured approach is dramatically easier to maintain.

### 3. Scales Our Expertise

- Junior staff can leverage senior knowledge embedded in templates
- Consistent methodology across all practitioners
- Knowledge stays with the firm, not individual practitioners

### 4. Adapts to Deal Specifics

The LLM layers handle what rules can't:
- Reading varied document formats
- Understanding context and implications
- Applying judgment on complexity adjustments
- Catching unusual situations

### 5. Transparent and Explainable

Every cost in the output can be traced:
- "This activity was identified because [LLM reasoning]"
- "Base cost of $X-$Y comes from template IDN-020"
- "Adjusted by 1.3x for financial services industry"

---

## Concerns Leadership May Have

### Concern 1: "Is this just AI making up numbers?"

**Response:** No. The architecture specifically prevents this.
- Stage 2 matching uses deterministic rules, not AI generation
- All cost ranges come from our maintained templates
- LLM adjustments are bounded (complexity multipliers, not arbitrary changes)
- Every number is traceable to a source

**Mitigation:** We track when LLM suggests activities not in our inventory - these get flagged for human review, not auto-costed.

### Concern 2: "How do we know the templates are accurate?"

**Response:** The templates are our team's knowledge codified.
- Initial ranges based on historical deal data and market rates
- Feedback loop captures when teams override estimates
- Regular calibration against actual project costs
- Templates improve over time with use

**Mitigation:** Built-in feedback tracker identifies templates that consistently need adjustment, generating recommendations for updates.

### Concern 3: "What if the AI misses something important?"

**Response:** Multiple safeguards:
- Stage 3 validation checks for common gaps
- Workstream coverage checks ensure nothing obvious is missed
- Human review remains part of the process
- System flags low-confidence situations

**Mitigation:** The tool augments expert judgment, doesn't replace it. Final review by experienced practitioners is expected.

### Concern 4: "Is this defensible to clients?"

**Response:** More defensible than pure expert judgment.
- Can show methodology documentation
- Can trace every cost to a template with market anchoring
- Can explain adjustments applied
- Consistent approach across engagements

**Mitigation:** Output includes full audit trail - which templates matched, what adjustments were applied, and why.

### Concern 5: "What about data security and confidentiality?"

**Response:** Architecture supports secure deployment.
- Can run with on-premise or private cloud LLMs
- Deal data doesn't need to leave controlled environment
- Templates contain no client-specific information

**Mitigation:** Deployment architecture will be reviewed with IT Security and Risk.

### Concern 6: "Will this replace our people?"

**Response:** No - it makes our people more effective.
- Handles tedious inventory and calculation work
- Frees practitioners to focus on judgment and client interaction
- Junior staff can produce senior-quality first drafts
- Experienced staff can review more deals

**Mitigation:** Position as productivity tool, not replacement. Success metric is deals supported, not headcount reduced.

---

## How This Evolves Over Time

### Phase 1: Foundation (Current)
- Build comprehensive activity inventory
- Establish three-stage architecture
- Test with sample scenarios
- Team review and calibration

### Phase 2: Pilot Deployment
- Use on 3-5 live deals with close supervision
- Capture feedback on accuracy and usability
- Refine templates based on actual results
- Document lessons learned

### Phase 3: Broader Rollout
- Train practitioners on methodology
- Establish template governance process
- Integrate feedback loop for continuous improvement
- Expand to additional deal types

### Phase 4: Advanced Capabilities
- Historical deal benchmarking
- Predictive analytics on deal complexity
- Integration with project management tools
- Automated progress tracking against estimates

### Continuous Improvement Loop

```
┌─────────────────┐
│  Use on Deals   │
└────────┬────────┘
         ▼
┌─────────────────┐
│ Capture Actuals │
│ vs. Estimates   │
└────────┬────────┘
         ▼
┌─────────────────┐
│ Identify Gaps   │
│ & Patterns      │
└────────┬────────┘
         ▼
┌─────────────────┐
│ Update Templates│
│ (Team Review)   │
└────────┬────────┘
         │
         └──────────► Back to Use on Deals
```

---

## PwC Proprietary Knowledge

This methodology and the underlying activity inventory represent **PwC proprietary intellectual property**, including:

- **Activity definitions** derived from hundreds of IT separation and integration engagements
- **Cost anchors** calibrated against actual project costs and market rates
- **Complexity modifiers** based on observed patterns across industries and deal sizes
- **TSA duration guidance** from negotiated agreements across our deal portfolio
- **Workstream interdependencies** learned from execution experience

This knowledge base provides competitive advantage:
- Faster, more accurate scoping
- Consistent quality across practitioners
- Defensible methodology for client discussions
- Continuous improvement through deal feedback

---

## Summary

The hybrid methodology combines:
- **LLM capabilities** for interpretation and context understanding
- **Structured templates** for consistent, auditable cost anchoring
- **Rule-based matching** for predictable, explainable results
- **Human expertise** for final validation and judgment

This approach:
- Eliminates AI hallucination risk on costs
- Makes updates easy (edit templates, not prompts)
- Scales our expertise across the practice
- Provides full audit trail for client discussions
- Improves over time through feedback loops

The activity inventory is the foundation - a codified, maintainable representation of our collective IT M&A experience that can be leveraged consistently across deals while continuously improving based on real-world results.

---

*Document Version: 1.0*
*Last Updated: January 2026*
*Classification: PwC Internal - Proprietary Methodology*
