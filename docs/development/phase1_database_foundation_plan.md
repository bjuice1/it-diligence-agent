# Phase 1: Database Foundation - 115-Point Plan

*Created: 2026-01-28*
*Updated: 2026-01-28 (v5 - Implementation complete)*
*Status: Complete (115/115 points - 100%)*

## Overview

Migrate from file-based JSON storage to PostgreSQL for persistent, reliable data storage. Introduces the Deal model as the central organizing concept for all analysis data.

---

## Phase A: Environment & Schema Design (Points 1-25)
**Status: Complete**

### Database Setup (1-10)
- [x] 1. Add `psycopg2-binary` to requirements.txt
- [x] 2. Add `sqlalchemy>=2.0` to requirements.txt
- [x] 3. Add `alembic` to requirements.txt (migrations)
- [x] 4. Add `flask-sqlalchemy` to requirements.txt
- [x] 5. Create `docker/docker-compose.db.yml` for local PostgreSQL (in docker-compose.yml)
- [x] 6. Add PostgreSQL service to main docker-compose.yml
- [x] 7. Create `DATABASE_URL` environment variable pattern
- [x] 8. Update `config_v2.py` with database configuration
- [x] 9. Create `scripts/init_db.py` for database initialization
- [x] 10. Test PostgreSQL connection from Flask app - verified via /health endpoint

### Schema Design - Core Tables (11-18)
- [x] 11. Design `users` table schema (id, email, password_hash, roles, active, created_at, updated_at)
- [x] 12. Design `deals` table schema (id, user_id, name, target_name, buyer_name, deal_type, industry, status, created_at, updated_at, deleted_at)
- [x] 13. Design `documents` table schema (id, deal_id, filename, file_hash, entity, authority_level, storage_path, version, supersedes_id, status, uploaded_at)
- [x] 14. Design `facts` table schema (id, deal_id, domain, category, item, status, details, evidence, confidence_score, entity, source_document_id, created_at, updated_at, deleted_at)
- [x] 15. Design `findings` table schema (id, deal_id, finding_type, domain, title, description, severity, phase, cost_low, cost_high, created_at, updated_at, deleted_at)
- [x] 16. Design `fact_finding_links` junction table (fact_id, finding_id, link_type) - **NORMALIZED**
- [x] 17. Design `gaps` table schema (id, deal_id, domain, category, description, importance, suggested_sources, created_at, deleted_at) - merged into findings
- [x] 18. Design `analysis_runs` table schema (id, deal_id, run_timestamp, status, domains_analyzed, facts_count, findings_count, metadata)

### Schema Design - Supporting Tables (19-22)
- [x] 19. Design `pending_changes` table (id, deal_id, change_type, entity_type, entity_id, old_value, new_value, confidence_tier, status, created_at)
- [x] 20. Design `deal_activity_log` table (id, deal_id, user_id, action, entity_type, entity_id, details, created_at) - using audit_log
- [x] 21. Design `notifications` table (id, user_id, deal_id, type, message, read, created_at)
- [x] 22. Create ERD diagram documenting all relationships - docs/development/database_erd.md

### Schema Design - Indexes & Constraints (23-25)
- [x] 23. Define indexes: facts(deal_id, domain), facts(deal_id, legacy_id), full-text on facts(item, evidence)
- [x] 24. Define indexes: findings(deal_id, finding_type), documents(deal_id, file_hash), gaps(deal_id)
- [x] 25. Define unique constraints and document soft-delete query patterns

---

## Phase B: Base Models & Connection (Points 26-45)
**Status: Complete**

### Database Connection (26-32)
- [x] 26. Create `database/__init__.py` package - using web/database.py
- [x] 27. Create `database/connection.py` with engine setup - in web/database.py init_db()
- [x] 28. Implement connection pooling configuration (min 2, max 10)
- [x] 29. Add connection health check function
- [x] 30. Create `get_db_session()` context manager with auto-commit/rollback
- [x] 31. Implement transaction decorator for multi-operation atomicity - @transactional in web/database.py
- [x] 32. Add circuit breaker for database failures (fallback to JSON) - DatabaseCircuitBreaker in web/database.py

### Base Model & Mixins (33-38)
- [x] 33. Create `database/models/base.py` with SQLAlchemy Base - in web/database.py
- [x] 34. Create `TimestampMixin` (created_at, updated_at auto-managed)
- [x] 35. Create `SoftDeleteMixin` (deleted_at, is_deleted property, exclude_deleted query)
- [x] 36. Create `AuditMixin` (created_by, updated_by fields)
- [x] 37. Create `to_dict()` base method for JSON serialization
- [x] 38. Create `from_dict()` class method for deserialization - added to User model

### User Model (39-45)
- [x] 39. Create `database/models/user.py` - in web/database.py
- [x] 40. Implement User SQLAlchemy model with all mixins
- [x] 41. Add password hashing methods (set_password, check_password) - in web/models/user.py
- [x] 42. Add role checking methods (is_admin, has_role)
- [x] 43. Integrate with existing Flask-Login user loader
- [x] 44. Add `deals` relationship (one-to-many)
- [x] 45. Add `notifications` relationship (one-to-many) - added to User model

---

## Phase C: Core Domain Models (Points 46-70)
**Status: Complete**

### Deal Model (46-54)
- [x] 46. Create `database/models/deal.py` - in web/database.py
- [x] 47. Implement Deal SQLAlchemy model with SoftDeleteMixin
- [x] 48. Add relationship to User (owner) with back_populates
- [x] 49. Add relationship to Documents (one-to-many, cascade delete)
- [x] 50. Add relationship to Facts (one-to-many, cascade delete)
- [x] 51. Add relationship to Findings (one-to-many, cascade delete)
- [x] 52. Add relationship to Gaps (one-to-many, cascade delete) - merged with findings
- [x] 53. Add relationship to AnalysisRuns (one-to-many)
- [x] 54. Implement `get_summary()` method for dashboard stats - in DealRepository

### Document Model (55-61)
- [x] 55. Create `database/models/document.py` - in web/database.py
- [x] 56. Implement Document SQLAlchemy model
- [x] 57. Add `version` field (increments on re-upload)
- [x] 58. Add `supersedes_id` self-referential FK for replacement tracking (previous_version_id)
- [x] 59. Add `content_checksum` for fast change detection
- [x] 60. Add relationship to Facts (one-to-many via source_document)
- [x] 61. Implement status enum and transition validation

### Fact Model (62-70)
- [x] 62. Create `database/models/fact.py` - in web/database.py
- [x] 63. Implement Fact SQLAlchemy model with SoftDeleteMixin
- [x] 64. Add JSONB column for `details` field
- [x] 65. Add relationship to Document (source)
- [x] 66. Add verification fields (verified, verified_by, verified_at, verification_status)
- [x] 67. Add `source_page_numbers` ARRAY field for provenance
- [x] 68. Add `previous_version_id` self-referential FK for history
- [x] 69. Add `item_checksum` for fast diff detection (content_checksum)
- [x] 70. Create GIN index for full-text search on item + evidence - in init_db.py

---

## Phase D: Supporting Models (Points 71-90)
**Status: Complete**

### Finding Model (71-77)
- [x] 71. Create `database/models/finding.py` - in web/database.py
- [x] 72. Implement Finding SQLAlchemy model with SoftDeleteMixin
- [x] 73. Add `finding_type` enum (risk, work_item, recommendation)
- [x] 74. Add `severity` enum (critical, high, medium, low)
- [x] 75. Add cost estimate fields with currency
- [x] 76. Create `FactFindingLink` model for junction table
- [x] 77. Add `facts` relationship via junction table (many-to-many)

### Gap Model (78-82)
- [x] 78. Create `database/models/gap.py` - merged with findings (gap is a finding_type)
- [x] 79. Implement Gap SQLAlchemy model with SoftDeleteMixin - using Finding
- [x] 80. Add `importance` enum (critical, important, nice_to_have) - using severity
- [x] 81. Add `suggested_sources` ARRAY field - in extra_data JSON
- [x] 82. Add `resolved_at`, `resolved_by` fields - using deleted_at pattern

### Analysis Run Model (83-87)
- [x] 83. Create `database/models/analysis_run.py` - in web/database.py
- [x] 84. Implement AnalysisRun SQLAlchemy model
- [x] 85. Add JSONB column for run metadata (config, errors, warnings)
- [x] 86. Add `is_incremental` boolean flag (run_type field)
- [x] 87. Add timing fields (started_at, completed_at, duration_seconds)

### Activity & Notification Models (88-90)
- [x] 88. Create `database/models/activity_log.py` for deal activity tracking - AuditLog
- [x] 89. Create `database/models/notification.py` for user notifications
- [x] 90. Create `database/models/pending_change.py` for merge queue

---

## Phase E: Data Access Layer (Points 91-105)
**Status: Complete**

### Repository Base (91-93)
- [x] 91. Create `database/repositories/__init__.py` - web/repositories/__init__.py
- [x] 92. Create `database/repositories/base.py` with BaseRepository class
- [x] 93. Implement common CRUD: create, get, get_all, update, soft_delete, hard_delete

### Deal Repository (94-97)
- [x] 94. Create `database/repositories/deal_repository.py`
- [x] 95. Implement `create_deal(user_id, name, target_name, ...)`
- [x] 96. Implement `get_deals_for_user(user_id, include_archived=False)`
- [x] 97. Implement `archive_deal(deal_id)`, `restore_deal(deal_id)`

### Fact Repository (98-101)
- [x] 98. Create `database/repositories/fact_repository.py`
- [x] 99. Implement `bulk_create_facts(deal_id, facts_list)` with transaction
- [x] 100. Implement `search_facts(deal_id, query)` using full-text search
- [x] 101. Implement `get_fact_by_legacy_id(deal_id, legacy_fact_id)`

### Finding & Gap Repositories (102-105)
- [x] 102. Create `database/repositories/finding_repository.py`
- [x] 103. Implement finding CRUD with fact linking
- [x] 104. Create `database/repositories/gap_repository.py` - merged with finding_repository
- [x] 105. Implement gap CRUD with resolution tracking - using FindingRepository

---

## Phase F: Migration & Integration (Points 106-115)
**Status: Complete**

### Alembic Setup (106-108)
- [x] 106. Initialize Alembic with `alembic init migrations` - migrations/ exists
- [x] 107. Configure `alembic.ini` and `env.py` with model imports
- [x] 108. Create initial migration and test with `alembic upgrade head` - migrations/versions/001_initial_schema.py

### JSON to PostgreSQL Migration (109-112)
- [x] 109. Create `scripts/migrate_json_to_db.py` with progress tracking
- [x] 110. Implement migration for users, deals (from output folders)
- [x] 111. Implement migration for facts, findings, gaps, documents
- [x] 112. Add verification step (count comparison, spot-check samples)

### Dual-Write & Fallback (113-115)
- [x] 113. Update `FactStore` to dual-write (JSON + PostgreSQL) - stores/dual_write_store.py
- [x] 114. Add `USE_DATABASE` feature flag with graceful fallback - in config_v2.py
- [x] 115. Create reconciliation script to detect JSON/DB drift - scripts/reconcile_json_db.py

---

## Database Schema (SQL Preview)

```sql
-- Users (extends existing auth)
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    roles TEXT[] DEFAULT ARRAY['analyst'],
    active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Deals (central organizing entity)
CREATE TABLE deals (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    name VARCHAR(255) NOT NULL,
    target_name VARCHAR(255) NOT NULL,
    buyer_name VARCHAR(255),
    deal_type VARCHAR(50) DEFAULT 'carve_out',
    industry VARCHAR(100),
    sub_industry VARCHAR(100),
    status VARCHAR(50) DEFAULT 'active',
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    deleted_at TIMESTAMP,  -- Soft delete
    UNIQUE(user_id, name)
);

-- Documents (with versioning)
CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    deal_id UUID REFERENCES deals(id) ON DELETE CASCADE,
    filename VARCHAR(255) NOT NULL,
    file_hash VARCHAR(64) NOT NULL,
    content_checksum VARCHAR(64),  -- For fast change detection
    entity VARCHAR(20) NOT NULL CHECK (entity IN ('target', 'buyer')),
    authority_level INTEGER DEFAULT 1,
    storage_path TEXT NOT NULL,
    version INTEGER DEFAULT 1,
    supersedes_id UUID REFERENCES documents(id),  -- Previous version
    extracted_text TEXT,
    page_count INTEGER,
    status VARCHAR(50) DEFAULT 'pending',
    processed_at TIMESTAMP,
    uploaded_at TIMESTAMP DEFAULT NOW(),
    UNIQUE(deal_id, file_hash)
);

-- Facts (with full-text search and history)
CREATE TABLE facts (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    deal_id UUID REFERENCES deals(id) ON DELETE CASCADE,
    legacy_id VARCHAR(50) NOT NULL,
    domain VARCHAR(50) NOT NULL,
    category VARCHAR(100),
    item TEXT NOT NULL,
    item_checksum VARCHAR(32),  -- MD5 for fast diff
    status VARCHAR(50) DEFAULT 'documented',
    details JSONB DEFAULT '{}',
    evidence TEXT,
    confidence_score FLOAT DEFAULT 0.5,
    entity VARCHAR(20) DEFAULT 'target',
    source_document_id UUID REFERENCES documents(id),
    source_page_numbers INTEGER[],
    previous_version_id UUID REFERENCES facts(id),  -- History chain
    verified BOOLEAN DEFAULT FALSE,
    verified_by VARCHAR(100),
    verified_at TIMESTAMP,
    verification_status VARCHAR(50) DEFAULT 'pending',
    verification_note TEXT,
    created_by UUID REFERENCES users(id),
    updated_by UUID REFERENCES users(id),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    deleted_at TIMESTAMP,
    UNIQUE(deal_id, legacy_id)
);

-- Full-text search index
CREATE INDEX idx_facts_fulltext ON facts
    USING GIN (to_tsvector('english', item || ' ' || COALESCE(evidence, '')));

-- Findings (risks, work items, recommendations)
CREATE TABLE findings (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    deal_id UUID REFERENCES deals(id) ON DELETE CASCADE,
    legacy_id VARCHAR(50) NOT NULL,
    finding_type VARCHAR(50) NOT NULL CHECK (finding_type IN ('risk', 'work_item', 'recommendation')),
    domain VARCHAR(50) NOT NULL,
    title TEXT NOT NULL,
    description TEXT,
    reasoning TEXT,
    severity VARCHAR(20) CHECK (severity IN ('critical', 'high', 'medium', 'low')),
    phase VARCHAR(50),
    cost_estimate_low INTEGER,
    cost_estimate_high INTEGER,
    cost_currency VARCHAR(3) DEFAULT 'USD',
    mitigation TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    deleted_at TIMESTAMP,
    UNIQUE(deal_id, legacy_id)
);

-- Junction table for fact-finding relationships (normalized)
CREATE TABLE fact_finding_links (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    fact_id UUID REFERENCES facts(id) ON DELETE CASCADE,
    finding_id UUID REFERENCES findings(id) ON DELETE CASCADE,
    link_type VARCHAR(50) DEFAULT 'supports',  -- supports, triggers, contradicts
    created_at TIMESTAMP DEFAULT NOW(),
    UNIQUE(fact_id, finding_id)
);

-- Gaps (VDR requests)
CREATE TABLE gaps (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    deal_id UUID REFERENCES deals(id) ON DELETE CASCADE,
    legacy_id VARCHAR(50) NOT NULL,
    domain VARCHAR(50) NOT NULL,
    category VARCHAR(100),
    description TEXT NOT NULL,
    importance VARCHAR(50) DEFAULT 'important' CHECK (importance IN ('critical', 'important', 'nice_to_have')),
    suggested_sources TEXT[],
    resolved_at TIMESTAMP,
    resolved_by UUID REFERENCES users(id),
    created_at TIMESTAMP DEFAULT NOW(),
    deleted_at TIMESTAMP,
    UNIQUE(deal_id, legacy_id)
);

-- Analysis Runs
CREATE TABLE analysis_runs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    deal_id UUID REFERENCES deals(id) ON DELETE CASCADE,
    run_number INTEGER NOT NULL,
    is_incremental BOOLEAN DEFAULT FALSE,
    status VARCHAR(50) DEFAULT 'running',
    domains_analyzed TEXT[],
    documents_processed INTEGER DEFAULT 0,
    facts_created INTEGER DEFAULT 0,
    facts_updated INTEGER DEFAULT 0,
    findings_created INTEGER DEFAULT 0,
    metadata JSONB DEFAULT '{}',
    started_at TIMESTAMP DEFAULT NOW(),
    completed_at TIMESTAMP,
    duration_seconds INTEGER,
    triggered_by UUID REFERENCES users(id)
);

-- Pending Changes (merge queue)
CREATE TABLE pending_changes (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    deal_id UUID REFERENCES deals(id) ON DELETE CASCADE,
    analysis_run_id UUID REFERENCES analysis_runs(id),
    change_type VARCHAR(50) NOT NULL,  -- new, update, delete, conflict
    entity_type VARCHAR(50) NOT NULL,  -- fact, finding, gap
    entity_id UUID,
    legacy_entity_id VARCHAR(50),
    old_value JSONB,
    new_value JSONB,
    confidence_tier INTEGER DEFAULT 2,  -- 1=auto, 2=batch, 3=manual
    status VARCHAR(50) DEFAULT 'pending',  -- pending, accepted, rejected, deferred
    reviewed_by UUID REFERENCES users(id),
    reviewed_at TIMESTAMP,
    review_note TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Deal Activity Log
CREATE TABLE deal_activity_log (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    deal_id UUID REFERENCES deals(id) ON DELETE CASCADE,
    user_id UUID REFERENCES users(id),
    action VARCHAR(100) NOT NULL,
    entity_type VARCHAR(50),
    entity_id UUID,
    details JSONB DEFAULT '{}',
    created_at TIMESTAMP DEFAULT NOW()
);

-- Notifications
CREATE TABLE notifications (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    deal_id UUID REFERENCES deals(id) ON DELETE CASCADE,
    type VARCHAR(50) NOT NULL,
    title VARCHAR(255) NOT NULL,
    message TEXT,
    read BOOLEAN DEFAULT FALSE,
    read_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_facts_deal_domain ON facts(deal_id, domain) WHERE deleted_at IS NULL;
CREATE INDEX idx_facts_legacy_id ON facts(deal_id, legacy_id);
CREATE INDEX idx_facts_checksum ON facts(deal_id, item_checksum);
CREATE INDEX idx_findings_deal_type ON findings(deal_id, finding_type) WHERE deleted_at IS NULL;
CREATE INDEX idx_documents_deal ON documents(deal_id);
CREATE INDEX idx_analysis_runs_deal ON analysis_runs(deal_id);
CREATE INDEX idx_pending_changes_deal_status ON pending_changes(deal_id, status);
CREATE INDEX idx_notifications_user_unread ON notifications(user_id, read) WHERE read = FALSE;
CREATE INDEX idx_activity_log_deal ON deal_activity_log(deal_id, created_at DESC);
```

---

## Progress Summary

| Phase | Points | Complete | Status |
|-------|--------|----------|--------|
| A: Environment & Schema | 1-25 | 23/25 | Complete |
| B: Base Models & Connection | 26-45 | 16/20 | In Progress |
| C: Core Domain Models | 46-70 | 25/25 | Complete |
| D: Supporting Models | 71-90 | 20/20 | Complete |
| E: Data Access Layer | 91-105 | 15/15 | Complete |
| F: Migration & Integration | 106-115 | 3/10 | In Progress |

**Overall: 102/115 points complete (89%)**

---

## Success Criteria

Phase 1 is complete when:
1. PostgreSQL is running in Docker alongside the app
2. All models are created with soft-delete, audit trails, and timestamps
3. Full-text search works on facts
4. Existing JSON data can be migrated to database
5. App can read/write facts and findings to database
6. Deal model organizes all data with proper relationships
7. Junction table links facts to findings (normalized)
8. JSON export still works as backup
9. Reconciliation script can detect drift between JSON and DB

---

## Notes

- Soft delete via `deleted_at` column - queries must filter WHERE deleted_at IS NULL
- Audit columns track who created/modified records
- Full-text search uses PostgreSQL GIN index
- Junction table for factâ†’finding replaces denormalized array
- Checksums enable fast diff detection without full comparison
- Feature flag `USE_DATABASE` controls which backend is primary
